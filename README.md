# CV Insight RAG (LangChain + Gradio)

CV Insight RAG is a lightweight prototype that lets you:

* Upload a candidate CV (PDF)
* Automatically extract key skills and seniority signals
* Ask natural language questions about the CV (e.g. "Does this person have Azure experience?", "Summarise their leadership experience")
* Get grounded answers backed only by the content of the uploaded CV

It uses Retrieval-Augmented Generation (RAG) under the hood: we embed the CV, index it, retrieve the most relevant chunks for every question, then ask an LLM to answer using just that evidence.

This is designed as a starter for:

* Recruiters / Talent teams
* Internal hiring tools
* AI-powered screening assistants
* ‚ÄúShow me how good you are with AI‚Äù demo projects

---

## üîç High-level Architecture

1. **Upload CV (PDF)**
   The file is parsed and converted to raw text.

2. **Chunk & Embed**
   The text is split into overlapping semantic chunks.
   Each chunk is embedded into a vector using `OpenAIEmbeddings` (via LangChain).

3. **Vector Store (FAISS)**
   All embeddings are stored in an in-memory FAISS index.
   We keep that index in session so you can query it.

4. **RAG Q&A**
   When you ask a question:

   * We embed the question
   * We retrieve the most relevant CV chunks
   * We pass those chunks + your question to an LLM (`ChatOpenAI`)
   * We generate an answer that cites the CV and refuses to hallucinate

5. **UI (Gradio)**

   * Skill/competency summary is shown instantly
   * You get an interactive chat interface for Q&A on that specific CV

---

## üß± Tech Stack

* **Python**
* **Gradio** ‚Äì quick interactive UI
* **LangChain**

  * `ChatOpenAI` for LLM responses
  * `OpenAIEmbeddings` for embeddings
  * `RetrievalQA` for the RAG chain
  * `FAISS` as the vector store
* **pypdf** ‚Äì PDF text extraction

This prototype currently assumes:

* You are okay using OpenAI for embeddings + generation.
* You are running a single-user, in-memory session (no auth, no persistence).

---

## üìÇ Project Structure

```text
cv-rag-app/
‚îú‚îÄ app.py                      # Gradio UI: upload CV, chat interface
‚îú‚îÄ rag_pipeline.py             # Ingestion + LangChain RetrievalQA chain factory
‚îú‚îÄ cv_parser.py                # Extracts text from PDF
‚îú‚îÄ profile_extract.py          # Heuristic skill/seniority summary for display
‚îú‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ .env.example                # Example for API keys
‚îî‚îÄ README.md                   # You're reading this
```

### `app.py`

* Defines the Gradio Blocks interface.
* Manages app state (`gr.State`) including:

  * The active QA chain for the uploaded CV
  * The running chat transcript

### `rag_pipeline.py`

Core logic:

* Extract text from PDF
* Chunk using `RecursiveCharacterTextSplitter`
* Embed with `OpenAIEmbeddings`
* Build a FAISS vectorstore
* Create a LangChain `RetrievalQA` chain using `ChatOpenAI`

Exposes:

* `ingest_cv_build_chain(file_obj)` ‚Üí builds that chain and returns session state
* `ask_question(session_state, question)` ‚Üí runs RAG to answer

### `cv_parser.py`

* Opens a PDF and returns cleaned text.

### `profile_extract.py`

* Heuristic (rule-based, token-free) guess of:

  * seniority (Lead / Senior / etc.)
  * tech skills / domains (Azure, Kafka, React, etc.)

Used to render the ‚ÄúCandidate Summary‚Äù panel after upload.

> This is intentionally lightweight and doesn‚Äôt cost tokens. You can swap it for an LLM-based profiler later.

---

## ‚öôÔ∏è Setup & Installation

### 1. Clone / copy the project

```bash
git clone <your-repo-url> cv-rag-app
cd cv-rag-app
```

(or just create the folder manually and drop these files in.)

### 2. Create a virtual environment (recommended)

```bash
python3 -m venv .venv
source .venv/bin/activate         # Windows: .venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure your OpenAI API key

This app currently uses:

* `ChatOpenAI` for answering
* `OpenAIEmbeddings` for vectorization

Both require `OPENAI_API_KEY`.

You can set it in your shell:

```bash
export OPENAI_API_KEY="sk-..."      # macOS / Linux
setx OPENAI_API_KEY "sk-..."        # Windows PowerShell
```

Or create a `.env` file based on `.env.example`:

```bash
cp .env.example .env
# then edit .env and paste your key
```

`langchain-openai` will automatically pick up `OPENAI_API_KEY` from environment variables.

---

## ‚ñ∂Ô∏è Run the app

From inside the virtual environment:

```bash
python app.py
```

Gradio will start a local web UI.
Open it in your browser.

You‚Äôll see:

* A file uploader for a PDF CV
* A ‚ÄúCandidate Summary‚Äù panel with detected skills & seniority guess
* A chat panel where you can ask questions about the CV

Example questions to try:

* ‚ÄúGive me a bullet-point summary of this candidate's cloud experience.‚Äù
* ‚ÄúHas this person built event-driven microservices?‚Äù
* ‚ÄúDoes the CV mention any leadership of teams or mentoring juniors?‚Äù
* ‚ÄúWrite me a short recruiter pitch for this candidate for a Senior Backend role.‚Äù

---

## üí¨ How Retrieval Works (RAG Flow)

Under the hood:

1. The PDF is parsed to text (`cv_parser.py`).
2. We split that text into overlapping chunks (800 chars, 200 char overlap) using `RecursiveCharacterTextSplitter`.
   This helps preserve context across bullet points and role descriptions.
3. We create embeddings for each chunk (vector representation) using `OpenAIEmbeddings`.
4. We store those embeddings in FAISS (in memory).
5. When you ask something:

   * We embed the question
   * We find the top ~4 most relevant chunks
   * We build a prompt:

     * Your question
     * Those chunks as ‚Äúcontext‚Äù
     * A system-style instruction that says ‚ÄúDo not make things up. If the CV doesn‚Äôt say it, say that.‚Äù
   * We send that to an LLM to generate the answer.

LangChain‚Äôs `RetrievalQA` chain handles the retrieval + prompt assembly + LLM call.

This means the model is grounded in the CV.
If the CV doesn‚Äôt include a skill, good answers should explicitly say `"The CV does not clearly provide that information."`

---

## üîê Notes / Limitations

* **No persistence yet.**
  When you upload a CV, it lives in memory for that session only.
  If you refresh, it‚Äôs gone.
  If you upload a new CV, it replaces the previous one.

* **Single-user assumption.**
  This prototype is not multi-tenant safe.
  If you run this on a server, all users share the same in-memory state.
  For production, you'd isolate per session/user, or persist indexes per CV.

* **Embeddings cost money.**
  Every upload will call embeddings on all chunks.
  Large CVs cost slightly more than short CVs.
  You can switch to local embeddings later (e.g. `sentence-transformers`) if you don‚Äôt want to call OpenAI.

* **We don't redact PII.**
  Candidate name, email, phone etc. are in the text and in the context passed to the model.
  For compliance you may want to add a redaction pass.
  You could also add a ‚ÄúGenerate anonymised CV‚Äù button (very easy with the current structure).

---

## üõ† Roadmap / Next Features

These are natural upgrades:

1. **Job Description Match**

   * Upload a JD alongside the CV
   * Create a second vector index for the JD
   * Ask: ‚ÄúHow well does this person match this role? Score / gaps / pitch.‚Äù

2. **Export Recruiter Pitch**

   * Button: ‚ÄúGenerate email pitch to hiring manager‚Äù
   * Output: bullet points + role fit + notice period (if stated in the CV)

3. **Multi-CV Compare**

   * Upload 2+ CVs
   * Ask: ‚ÄúWho has stronger Azure experience?‚Äù

4. **Persistence**

   * Store FAISS indexes on disk using `FAISS.save_local()` / `FAISS.load_local()`
   * Give each CV an ID and reload them later

5. **Local / Private Mode**

   * Swap `OpenAIEmbeddings` for `HuggingFaceEmbeddings`
   * Swap `ChatOpenAI` for a local model via `langchain_community.llms.Ollama`
   * Result: fully offline / on-prem mode for privacy-sensitive clients

---

## ‚ùì FAQ

**Q: Do you retrain a model on the CV?**
A: No. We‚Äôre not fine-tuning. We‚Äôre doing retrieval: embed CV ‚Üí search ‚Üí feed only relevant passages to the LLM at question time.

**Q: Can it hallucinate?**
A: It's less likely because we explicitly tell it ‚ÄúOnly answer using this context.‚Äù
If the info is missing, it should say so. Still, you should not treat the output as a legal/HR decision on its own.

**Q: Can I run this offline without OpenAI?**
A: Not in this base version.
But yes, architecturally it‚Äôs easy:

* Replace `OpenAIEmbeddings` with `HuggingFaceEmbeddings`
* Replace `ChatOpenAI` with a local LLM via Ollama or similar
* Everything else stays the same

---

## ‚úÖ TL;DR for stakeholders

* Upload a PDF CV.
* The app builds a private knowledge base from that CV.
* You can ask targeted questions and get grounded answers.
* There‚Äôs no DB, no infra ‚Äî just `python app.py`.

This is the minimal working skeleton for an AI recruiter assistant / talent screener / CV intelligence tool, built with LangChain + Gradio.

## FastAPI API (service entrypoint)

- Run locally: `OPENAI_API_KEY=... uvicorn api.main:app --reload --port 8000`
- Endpoints:
  - POST /ingestions (multipart file) -> { document_id }
  - POST /queries { document_id, question } -> { answer }

## Persistence & Multi-CV

The application now supports persistence and multi-CV management:

*   **Persistence:** FAISS indexes are saved to the `data/` directory (or `DATA_DIR` env var).
*   **Multi-CV:** You can upload multiple CVs. Each is assigned a unique ID.
*   **Selection:** Use the dropdown in the UI to switch between previously uploaded candidates.
*   **API:**
    *   `GET /documents`: List all available document IDs.
    *   `POST /ingestions`: Upload and index a new CV.
    *   `POST /queries`: Query a specific document by ID.
